---
Titanic Death Prediction Model
==============================
author: "Peter"

Setup
-----
Set working directory to place with files and load files
  setwd('C:/Users/Pete/titanic-kaggle')
  train <- read.csv("C:/Users/Pete/titanic-kaggle/train.csv")
  View(train)
  test <- read.csv("C:/Users/Pete/titanic-kaggle/test.csv")
  View(test)

First look at Data
------------------
Basic note: test has 418 observations and 11 variables.
train has 891 observations and 12 variables.
The 1 less variable is whether the passengers survived or not.
Create a model using train and then test the model to see who in "test" survived.

```{r}
str(train)
#3 data types here, int, num, and factor. int is integer, num is floating, and
#factor is a category, which can hold strings.
table(train$Survived)
#This outputs a table, row1 is [0,1], row2 is [549, 342]. as in 549 died and 342 survived
prop.table(table(train$Survived))
#Gives proportions, and we see that 61.61% of passengers died while 38.38% survived in the training set
```
1st Hypothesis
--------------
As a first prediction, since the majority of patients died in my train set, let us say everyone died.

```{r}
test$Survived <- rep(0,418)
```
This creates a new column called "Survived" and inputs 0 for 418 rows.
If a "Survived" column already existed, then the values would be replaced.
```{r}
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
#These words are case sensitive
write.csv(submit,file = "Titanic_Model.csv",row.names = FALSE)
#Create csv file using data frame and exclude row numbers
#Submission to Kaggle requires passengerID and Survived prediction, 2 columns with 418 observations.
```
Results from first submission:

Prediction score of: 0.62679
- - - -
Gender-Based analysis
---------------------
```{r}
summary(train$Sex)
#female 314, male 577
prop.table(table(train$Sex,train$Survived),1)
#The 1 makes it so that I get row-wise proportions, 2 gives column-wise proportions
#74% of females survived, while 81% of males died
```

```{r}
test$Survived <- 0
#Creates a whole column of 0
test$Survived[test$Sex == 'female'] <- 1
#From everyone died, change so that only men died. All women survived.
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit,file = "Titanic_Model.csv",row.names = FALSE)
```

Prediction score of: 0.76555
- - - -
Age-based Analysis
------------------
```{r}
summary(train$Age)
#Mean of 29.70, with 177 NA's
train$Child <- 0
train$Child[train$Age <18] <- 1
#Create a new column to identify children, who are below 18, and assume they all survived.
aggregate(Survived ~ Child + Sex, data = train, FUN = sum)
#aggregate(target variable ~ subset variable, data frame, function)
#38 female children survived, 23 male children survived
aggregate(Survived ~ Child + Sex, data=train, FUN=length)
#Length of the vector. For example, child0sexfemale has survived length of 259
aggregate(Survived ~Child + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#this gives proportion of survival subset by children and sex
#More females survived, regardless of whether they were children or not, and so the original prediction that females always had a higher probability of surviving does not change.
```
Abandon Age-Based Analysis as is because it is fruitless.


Fare-Class-Based Analysis
-------------------------
```{r}
train$Fare2 <- '30+'
train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'
train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'
train$Fare2[train$Fare < 10] <- '<10'
#Fare is a continuous variable, and so create bins

aggregate(Survived~Fare2 + Pclass + Sex, data=train,FUN=function(x) {sum(x)/length(x)})
#Survival proportions based on gender, fare cost, and pclass
#Majority of males, regardless of class or fare died. However, most of the class 3 woomen who paid more than $20 also died. A hypothesis could be that these expensive cabins were located close to the iceberg impact site, or were further from the exit stairs.
```

```{r}
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
test$Survived[test$Sex == 'female' & test$Pclass == 3 & test$Fare >= 20] <- 0
submit <- data.frame(PassengerId = test$PassengerId,Survived = test$Survived)
write.csv(submit,file="Titanic_Model.csv",row.names=FALSE)
```
Over the previous model, females who are in class 3 and has a fare greater than 20 are assumed to die.
Thus far, I cut apart the data into component so that I could identify which group died and which did not.

Prediction score of: 0.77990
- - - -

Analysis using Decision Trees
-----------------------------
Decision trees are able to quickly go through the data and provide a 0% or 100% chance of survival. However, decision trees are not optimal by themselves and are prone to overfitting.

```{r}
library(rpart)
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, method="class")
```
Feed rpart with variable of interest(survived) with variables used for preiction, and then the data. For method, use "class" for 1 or 0, and use "anova" for continuous variables such as age
```{r}
plot(fit)
text(fit)
#this plot is neither insightful nor aesthetic
library(rattle)
library(rpart.plot)
fancyRpartPlot(fit)
#visualizing the decision tree
Prediction <- predict(fit,test,type = "class")
submit <- data.frame(PassengerId = test$PassengerId,Survived = Prediction)
write.csv(submit, file = "Titanic_Model.csv",row.names=FALSE)
```
Fit was created using the train data. Based on the logic of the train data, a prediction model was created using the predict function (which comes from  rpart) and inputting it with fit (which holds the rules) and test (the data set we're interested in)

Using decision trees, I explored areas I previously didn't consider. However, because the training set is different from the test set, I only achieved modest gains. I need a new technique to achieve greater predictability.


Prediction score of: 0.78469
- - - -
Feature Engineering
-------------------
Feature engineering is important for a machine learning algorithm because an engineered feature may be easier to digest and make rules from than the variables that the feature was derived from. Dive into existing variables and extract parts of them to build a new predictive attribute.

train$Name[1]


1st Prediction: Persons title can provide insight
-----------------------------------------------------
Use rbind, row bind, to merge 2 data frames. This can be done so long as both of the data frames have te same columns as each other.

```{r}
test$Survived <- NA
combi <- rbind(train,test)
```
combi is a dataframe with all of the same rows as the original 2 datasets. train is first, test second.
```{r}
combi$Name[1]
#Name is automatically set as a factor, but want strings.
combi$Name <- as.character(combi$Name)
combi$Name[1]
#Name is now a string, and the 1307 levels have disappeared. We have pure text.
#Output is "Braund, Mr. Owen Harris"
strsplit(combi$Name[1],split='[,.]')
#Split the string from , and . --> this is a regex
strsplit(combi$Name[1],split='[,.]')[[1]] #strsplit uses a doubly stacked matrix
strsplit(combi$Name[1],split='[,.]')[[1]][2] #Extract the title.

combi$Title <- sapply(combi$Name,FUN=function(x) {strsplit(x,split='[,.]')[[1]][2]})
#Feed sapply vector of names and function that I want. sapply runs through the rows of the vector of names, and sends each name to the fnuction. The results are combined into a vector and then the new vector is stored in the column Title in the dataframe combi.

combi$Title <- sub(' ','',combi$Title) #replaces all spaces with empty string
```
```{r}
table(combi$Title)
```
Have titles such as Capt, Major, Miss, Mr, Mrs, Sir, Lady, Dr, etc.
Since there are some useless titlees though, combine them. For example, Mademoiselle and Madame are the same.
```{r}
combi$Title[combi$Title %in% c('Mme','Mlle')] <- 'Mlle'
```
I am consolidating some categories.
%in% operator checks to see if a value is part of the vector I am comparing it to. I am combining 2 titles, 'Mme' and 'Mlle', into a new temporary vector using the c() operator and seeing if any of the existing titles in the entire Title column match either of them. I then replace any match with 'Mlle'.
```{r}
table(combi$Title) # Mlle used to be 2, Mme 1, now Mme is gone and Mlle is 3
```
Combine the rich/military titles for men and women
```{r}
combi$Title[combi$Title %in% c('Capt','Don','Major','Sir')] <- 'Sir'
combi$Title[combi$Title %in% c('Dona','Lady','the Countess','Jonkheer')] <- 'Lady'
#Now need to change variable type back to a factor
combi$Title <- factor(combi$Title)
```
Done with passenger's title.
2nd Prediction: Families live and die together.
-----------------------------------------------
Now consolidate SibSb and Parch, variables that indicate the number of family members the passenger is travelling with.
```{r}
combi$FamilySize <- combi$SibSp + combi$Parch + 1
```
Add the number of siblings, spouses, parents and children the passenger had with them plus 1 for self.
The idea is that families move together, and so if one member isn't able to survive, the rest would be more likely to not survive.

```{r}
combi$Surname <- sapply(combi$Name,FUN=function(x) {strsplit(x,split='[,.]')[[1]][1]})
combi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname,sep="")
combi$FamilyID[combi$FamilySize <= 2] <- 'Small' #Disinction between big and small families
famIDs <- data.frame(table(combi$FamilyID))
famIDs <- famIDs[famIDs$Freq <= 2,]
combi$FamilID[combi$FamilyID %in% famIDs$Var1] <- 'Small'
combi$FamilyID <- factor(combi$FamilyID)
train <- combi[1:891,] #take all columns of the specified rows
test <- combi[892:1309,]
```
Finished with 2 engineered variables. Title, and FamilySize as in Title has influence on survival, and FamilySize has influence on survival.


Finally: Creating the fit rules and running the decision tree.
--------------------------------------------------------------
```{r}
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID, data = train,method = "class")
```
rpart is for recursive partitioning and regression trees
```{r}
?predict
Prediction <- predict(fit,test,type = "class")
submit <- data.frame(PassengerId = test$PassengerId,Survived = Prediction)
write.csv(submit, file = "Titanic_Model.csv",row.names=FALSE)
```
Final Notes
-----------
Decision trees are biased to favor factors with many levels. The reason is so that the decision node can chop and change the data into the best way possible combnation for the nodes.

To progress, create more engineered variables. Analyze wiht moe complexity parameters and trim deeper trees. In addition, consider excluding some variables from the tree.

However, the nature of the decision tre is clear. Title or gender variables will govern the first decision due to the greedy nature of decision trees. The bias towards many-levelled factors and overfitting will also reoccuring problems.

Prediction score of: 0.79904
- - - -
