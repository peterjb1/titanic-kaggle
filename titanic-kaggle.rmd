---
Titanic Death Prediction Model
==============================
author: "Peter"

Setup
-----
Set working directory to place with files and load files
  setwd('C:/Users/Pete/titanic-kaggle')
  train <- read.csv("C:/Users/Pete/titanic-kaggle/train.csv")
  View(train)
  test <- read.csv("C:/Users/Pete/titanic-kaggle/test.csv")
  View(test)

First look at Data
------------------
Basic note: test has 418 observations and 11 variables.
train has 891 observations and 12 variables.
The 1 less variable is whether the passengers survived or not.
Create a model using train and then test the model to see who in "test" survived.

```{r}
str(train)
#3 data types here, int, num, and factor. int is integer, num is floating, and
#factor is a category, which can hold strings.
table(train$Survived)
#This outputs a table, row1 is [0,1], row2 is [549, 342]. as in 549 died and 342 survived
prop.table(table(train$Survived))
#Gives proportions, and we see that 61.61% of passengers died while 38.38% survived in the training set
```
1st Hypothesis
--------------
As a first prediction, since the majority of patients died in my train set, let us say everyone died.

```{r}
test$Survived <- rep(0,418)
```
This creates a new column called "Survived" and inputs 0 for 418 rows.
If a "Survived" column already existed, then the values would be replaced.
```{r}
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
#These words are case sensitive
write.csv(submit,file = "Titanic_Model.csv",row.names = FALSE)
#Create csv file using data frame and exclude row numbers
#Submission to Kaggle requires passengerID and Survived prediction, 2 columns with 418 observations.
```
Results from first submission:

Prediction score of: 0.62679
- - - -
Gender-Based analysis
---------------------
```{r}
summary(train$Sex)
#female 314, male 577
prop.table(table(train$Sex,train$Survived),1)
#The 1 makes it so that I get row-wise proportions, 2 gives column-wise proportions
#74% of females survived, while 81% of males died
```

```{r}
test$Survived <- 0
#Creates a whole column of 0
test$Survived[test$Sex == 'female'] <- 1
#From everyone died, change so that only men died. All women survived.
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit,file = "Titanic_Model.csv",row.names = FALSE)
```

Prediction score of: 0.76555
- - - -
Age-based Analysis
------------------
```{r}
summary(train$Age)
#Mean of 29.70, with 177 NA's
train$Child <- 0
train$Child[train$Age <18] <- 1
#Create a new column to identify children, who are below 18, and assume they all survived.
aggregate(Survived ~ Child + Sex, data = train, FUN = sum)
#aggregate(target variable ~ subset variable, data frame, function)
#38 female children survived, 23 male children survived
aggregate(Survived ~ Child + Sex, data=train, FUN=length)
#Length of the vector. For example, child0sexfemale has survived length of 259
aggregate(Survived ~Child + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#this gives proportion of survival subset by children and sex
#More females survived, regardless of whether they were children or not, and so the original prediction that females always had a higher probability of surviving does not change.
```
Abandon Age-Based Analysis as is because it is fruitless.


Fare-Class-Based Analysis
-------------------------
```{r}
train$Fare2 <- '30+'
train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'
train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'
train$Fare2[train$Fare < 10] <- '<10'
#Fare is a continuous variable, and so create bins

aggregate(Survived~Fare2 + Pclass + Sex, data=train,FUN=function(x) {sum(x)/length(x)})
#Survival proportions based on gender, fare cost, and pclass
#Majority of males, regardless of class or fare died. However, most of the class 3 woomen who paid more than $20 also died. A hypothesis could be that these expensive cabins were located close to the iceberg impact site, or were further from the exit stairs.
```

```{r}
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
test$Survived[test$Sex == 'female' & test$Pclass == 3 & test$Fare >= 20] <- 0
submit <- data.frame(PassengerId = test$PassengerId,Survived = test$Survived)
write.csv(submit,file="Titanic_Model.csv",row.names=FALSE)
```
Over the previous model, females who are in class 3 and has a fare greater than 20 are assumed to die.
Thus far, I cut apart the data into component so that I could identify which group died and which did not.

Prediction score of: 0.77990
- - - -

Analysis using Decision Trees
-----------------------------
Decision trees are able to quickly go through the data and provide a 0% or 100% chance of survival. However, decision trees are not optimal by themselves and are prone to overfitting.

```{r}
library(rpart)
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, method="class")
```
Feed rpart with variable of interest(survived) with variables used for preiction, and then the data. For method, use "class" for 1 or 0, and use "anova" for continuous variables such as age
```{r}
plot(fit)
text(fit)
#this plot is neither insightful nor aesthetic
library(rattle)
library(rpart.plot)
fancyRpartPlot(fit)
#visualizing the decision tree
Prediction <- predict(fit,test,type = "class")
submit <- data.frame(PassengerId = test$PassengerId,Survived = Prediction)
write.csv(submit, file = "Titanic_Model.csv",row.names=FALSE)
```
Fit was created using the train data. Based on the logic of the train data, a prediction model was created using the predict function (which comes from  rpart) and inputting it with fit (which holds the rules) and test (the data set we're interested in)

Using decision trees, I explored areas I previously didn't consider. However, because the training set is different from the test set, I only achieved modest gains. I need a new technique to achieve greater predictability.


Prediction score of: 0.78469
- - - -
Feature Engineering
-------------------
Feature engineering is important for a machine learning algorithm because an engineered feature may be easier to digest and make rules from than the variables that the feature was derived from. Dive into existing variables and extract parts of them to build a new predictive attribute.

train$Name[1]


1st Prediction: Persons title can provide insight
-----------------------------------------------------
Use rbind, row bind, to merge 2 data frames. This can be done so long as both of the data frames have te same columns as each other.

```{r}
test$Survived <- NA
combi <- rbind(train,test)
```
combi is a dataframe with all of the same rows as the original 2 datasets. train is first, test second.
```{r}
combi$Name[1]
#Name is automatically set as a factor, but want strings.
combi$Name <- as.character(combi$Name)
combi$Name[1]
#Name is now a string, and the 1307 levels have disappeared. We have pure text.
#Output is "Braund, Mr. Owen Harris"
strsplit(combi$Name[1],split='[,.]')
#Split the string from , and . --> this is a regex
strsplit(combi$Name[1],split='[,.]')[[1]] #strsplit uses a doubly stacked matrix
strsplit(combi$Name[1],split='[,.]')[[1]][2] #Extract the title.

combi$Title <- sapply(combi$Name,FUN=function(x) {strsplit(x,split='[,.]')[[1]][2]})
#Feed sapply vector of names and function that I want. sapply runs through the rows of the vector of names, and sends each name to the fnuction. The results are combined into a vector and then the new vector is stored in the column Title in the dataframe combi.

combi$Title <- sub(' ','',combi$Title) #replaces all spaces with empty string
```
```{r}
table(combi$Title)
```
Have titles such as Capt, Major, Miss, Mr, Mrs, Sir, Lady, Dr, etc.
Since there are some useless titlees though, combine them. For example, Mademoiselle and Madame are the same.
```{r}
combi$Title[combi$Title %in% c('Mme','Mlle')] <- 'Mlle'
```
I am consolidating some categories.
%in% operator checks to see if a value is part of the vector I am comparing it to. I am combining 2 titles, 'Mme' and 'Mlle', into a new temporary vector using the c() operator and seeing if any of the existing titles in the entire Title column match either of them. I then replace any match with 'Mlle'.
```{r}
table(combi$Title) # Mlle used to be 2, Mme 1, now Mme is gone and Mlle is 3
```
Combine the rich/military titles for men and women
```{r}
combi$Title[combi$Title %in% c('Capt','Don','Major','Sir')] <- 'Sir'
combi$Title[combi$Title %in% c('Dona','Lady','the Countess','Jonkheer')] <- 'Lady'
#Now need to change variable type back to a factor
combi$Title <- factor(combi$Title)
```
Done with passenger's title.
2nd Prediction: Families live and die together.
-----------------------------------------------
Now consolidate SibSb and Parch, variables that indicate the number of family members the passenger is travelling with.
```{r}
combi$FamilySize <- combi$SibSp + combi$Parch + 1
```
Add the number of siblings, spouses, parents and children the passenger had with them plus 1 for self.
The idea is that families move together, and so if one member isn't able to survive, the rest would be more likely to not survive.

```{r}
combi$Surname <- sapply(combi$Name,FUN=function(x) {strsplit(x,split='[,.]')[[1]][1]})
combi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname,sep="")
combi$FamilyID[combi$FamilySize <= 2] <- 'Small' #Disinction between big and small families
famIDs <- data.frame(table(combi$FamilyID))
famIDs <- famIDs[famIDs$Freq <= 2,]
combi$FamilyID[combi$FamilyID %in% famIDs$Var1] <- 'Small'
combi$FamilyID <- factor(combi$FamilyID)
train <- combi[1:891,] #take all columns of the specified rows
test <- combi[892:1309,]
```
Finished with 2 engineered variables. Title, and FamilySize as in Title has influence on survival, and FamilySize has influence on survival.


Finally: Creating the fit rules and running the decision tree.
--------------------------------------------------------------
```{r}
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID, data = train,method = "class")
```
rpart is for recursive partitioning and regression trees
```{r}
?predict
Prediction <- predict(fit,test,type = "class")
submit <- data.frame(PassengerId = test$PassengerId,Survived = Prediction)
write.csv(submit, file = "Titanic_Model.csv",row.names=FALSE)
```
Final Notes
-----------
Decision trees are biased to favor factors with many levels. The reason is so that the decision node can chop and change the data into the best way possible combnation for the nodes.

To progress, create more engineered variables. Analyze wiht moe complexity parameters and trim deeper trees. In addition, consider excluding some variables from the tree.

However, the nature of the decision tre is clear. Title or gender variables will govern the first decision due to the greedy nature of decision trees. The bias towards many-levelled factors and overfitting will also reoccuring problems.

Prediction score of: 0.79904
- - - -

Random Forests
--------------

Ensemble models: Grow a lot of different models and then take the average of the outcomes.
This can overcome the problem of overfitting that comes with decision trees. 

The formulas for building a single decision tree are the same every time, and so randomness needs to be introduced.

There are 2 ways to introduce randomness.

1. Bagging
-----------
Bagging is bootstrap aggregating. Bagging takes a randomized sample of the rows in my training set, with replacement. 

For example: Want to perform bagging on a training set with 10 rows.
```{r}
sample(1:10,replace=TRUE)
```
With bagging, each decision will grow slightly differently, unless a feature is very strong such as gender.
2. Take subset of available variables
-------------------------------------
Instead of looking at the entire pool of available variables, Random Forest tke only a subset of them. The selection of available variables is canged for each nd every node in the decision trees. Many trees won't even have the gender variable available at the fist split due to this.

Before proceeding with Random Forests, it is required to clean up the data, as in replace all values such as NA.

Data cleaning
--------------
```{r}
summary(combi$Age)
Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + FamilySize,
data = combi[!is.na(combi$Age),],method="anova")
combi$Age[is.na(combi$Age)] <- predict(Agefit,combi[is.na(combi$Age),])
```
263 age values are missing, and so fill them up using a prediction. Use method "anova" since now I want a continuous variable.
```{r}
summary(combi)
```
After getting a high level view, I see that Fare has 1 NA and Embarked has a blank for 2 passengers.
I don't know where the 2 blanks are from, but assume S (Southhampton) because it is the overwhelming location people are from.
```{r}
which(combi$Embarked == '')
#62 and 830 are the missing passeners
combi$Embarked[c(62,830)] = "S"
combi$Embarked <- factor(combi$Embarked)
#Embarked is now fixed.
which(is.na(combi$Fare))
combi$Fare[1044] <- median(combi$Fare, na.rm=TRUE)
#Replace the NA with the median fare value
```
Data is now cleared of NAs

32 levels max for a factor
--------------------------
Random Forests in R can only handle factors with up to 32 levels. 
```{r}
levels(combi$FamilyID)
#FamilyID has 97 levels, so it must be trimmed.
combi$FamilyID2 <- combi$FamilyID
combi$FamilyID2 <- as.character(combi$FamilyID2)
combi$FamilyID2[combi$FamilySize <= 3] <- 'Small'
combi$FamilyID2 <- factor(combi$FamilyID2)
levels(combi$FamilyID2) # FamilyID2 has 31 levels
```

```{r}
library(randomForest)
#set seed in R before beginning to make results reproducible next time you load the code up
set.seed(499)
fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID2, data=train,importance=TRUE,ntree=2000)
#ntree = # of trees to grow
#importance allows inspection of variable importance
varImpPlot(fit)
#Hierarchy shows he importance of a variable. Title is at the top as expected.
#this random forest prediction will perform less than the feature engineered prediction.
```
library(party)

```{r}
#try forest of conditional inference trees
fit <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + Fare + SibSp + Parch + Embarked + Title + FamilySize + FamilyID, data = train, controls = cforest_unbiased(ntree=2000,mtry=3))
#conditonal inference trees can handle more levels than andom forests
Prediction <- predict(fit,test,OOB=TRUE,type="response")
submit <- data.frame(PassengerId = test$PassengerId,Survived = Prediction)
write.csv(submit, file = "Titanic_Model.csv",row.names=FALSE)
```
Exclude Parch test: 0.80861
Exclude Parch and SibSp: 0.81340
Exclude Parch, SibSp, and Embarked: 0.80861
Exclude Parch and FamilySize: 0.81340
Exclude Parch and FamilyID: 0.79904 

I achieved the highest prediction score of 0.81340.
It appears that conditional inference trees is more powerful than random forests. In addition, creating the feature engineering has been very important in achieveing my prediction score. By creating new variables and running conditional inference trees with multiple prediction models, I can run many simulations to identify which is most accurate.

![Final Score](http://i.imgur.com/Iu20CCh.jpg)

This has been a fun project. I learned about feature engineering, random forests, and the importance of being creative instead of trying to brute force with complex algorithms.